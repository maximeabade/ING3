{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0efd299e",
   "metadata": {},
   "source": [
    "\n",
    "<h1 style=\"border: 2px solid black; padding: 15px; border-radius: 12px;\" align='center'>IA pour la cyber</h1>    \n",
    "\n",
    "<h2 align='center'> Analyse de textes en Python : premier pas </h2>\n",
    "\n",
    "<h3 align='center'> Jordy Palafox </h3>\n",
    "<h3 align='center'> Ing3 CS - 2024/2025 </h3>\n",
    "      \n",
    "      \n",
    "<div style=\"display:flex\"> \n",
    "    <img src=\"cytech.png\", style=\"width:250px;height:50\"> \n",
    "    <img src=\"cy.jpg\", style=\"width:300px;height:100px\"> \n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107a548c",
   "metadata": {},
   "source": [
    "Inspiré largement du livre Python pour le Data Scientist, Emmanuel Jakobowicz, Edition Dunod.\n",
    "\n",
    "\n",
    "Quelques outils classiques sur l'analyse textuelle sont :\n",
    "+ *String* , interne à Python pour des actions particulières sur la class str\n",
    "+ *NLTK* (Natural Language Toolkit) pour le prétraitement de textes avec fonctions et bases de données particulières\n",
    "+ *Scikit-Learn* pour une approche statistique des transformatins\n",
    "+ *Spacy* https://spacy.io/ qui vient compléter NLTK, plus récent et très complet, avec  des modèles etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77169aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea15d99",
   "metadata": {},
   "source": [
    "# Prétraitrement des données\n",
    "\n",
    "Ou comment transformer des données brutes en des données exploitables, structurées.\n",
    "\n",
    "## Etape 1 : la tokenisation\n",
    "\n",
    "On va récupérer quelques données (Scraping) de Wikipédia à l'aide de la libraire Beautiful-Soup https://www.crummy.com/software/BeautifulSoup/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "347e8268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag not found\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "# On récupère maintenant les données d'une page\n",
    "reponse = urllib.request.urlopen('https://fr.wikipedia.org/wiki/Python_(langage)')\n",
    "\n",
    "# extraction du texte en html\n",
    "html = reponse.read()\n",
    "\n",
    "# On utilise un objet de la classe BeautifulSoup pour traiter du code html\n",
    "soup = BeautifulSoup(html, 'html5lib')\n",
    "\n",
    "# on récupère tout le code à partir de la balise dont la suite nous intéresse\n",
    "tag = soup.find('div', {'class' : 'mw-parset-output'})\n",
    "\n",
    "# on extrait le texte du code si la balise est trouvée\n",
    "if tag:\n",
    "    text = tag.text\n",
    "    print(type(text), len(text), sep=',')\n",
    "else:\n",
    "    print(\"Tag not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4fad90",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"On a donc récupéré une chaine de caractères composés de {len(text)} catactères\".\n",
    "\n",
    "# Il faut maintenant extraire les mots de cette chaîne\n",
    "\n",
    "tokens = ntlk.word_tokenize(text.lower(), language='french')\n",
    "print(type(tokens), len(tokens))\n",
    "\n",
    "f\"On a une liste de {len(tokens)} tokens donc de groupe de caractères en minuscule\"\n",
    "\n",
    "# On regarde souvent les fréquences de ces tokens\n",
    "\n",
    "freq = ntlk.FreqDist(tokens)\n",
    "\n",
    "# Les 10 mots les plus communs \n",
    "freq.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5027b3d4",
   "metadata": {},
   "source": [
    "## Etape 2 : Nettoyer le texte\n",
    "\n",
    "On voit que le texte donc de nombreux mots qui ne servent pas à le distinguer (les articles par exemple)\n",
    "Ces mots sont appelés **stopwords** et généralement on les supprime.\n",
    "Les libraires comme NLTK ou SPacy proposent des dictionnaires de stopwords pour différentes langagues.\n",
    "\n",
    "Il y a aussi le problème de la ponctuation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77284d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Créons une liste de mots + mots que l'on ajoute nous même + ponctuation\n",
    "\n",
    "sr = stopwords.words('french') + ['les', 'a', 'il', '+', ',','<', '>', \"''\"]+list(string.punctuation)\n",
    "\n",
    "# On fait le ménage dans les tokens \n",
    "\n",
    "tokens_propres = [i for in tokens if i not in sr]\n",
    "\n",
    "# Puis on regarde à nouveau les mots les plus fréquents\n",
    "freq = ntlk.FreqDist(tokens_propres)\n",
    "freq.plot(20, tile = 'Fréquence des mots dans la page')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20980ab",
   "metadata": {},
   "source": [
    "D'autres outils utiles : sur ntlk, *sent_tokenize* extrait les phrases, WordNet permet de récupérer les synonymes en francais et réduire ainsi le nombre de variables.\n",
    "\n",
    "\n",
    "Nous allons pouvoir mettre maintenant en place un modèle prédictif puisqu'on sait traiter des données !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64898c6",
   "metadata": {},
   "source": [
    "# Premier modèle prédictif pour du NLP\n",
    "\n",
    "On va utiliser un exemple concret, des données (téléchargeables ici https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset ) pour construire un filtre anti spam de sms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4273b450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Lecture des données\n",
    "sms = pd.read_table( ' ..... ')\n",
    "\n",
    "# Il est intéressant de regarder les labels donc le nombre de sms non spams et de spams \n",
    "# il y a souvent la problématique de rencontrer des classes déséquilibrées qu'il faut prendre en compte\n",
    "\n",
    "sms['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0c4eef",
   "metadata": {},
   "source": [
    "Les données sont textuelles donc non structurées. Pour leur donner une structure nous allons utiliser les deux mêmes suivantes : \n",
    "+ *CountVectorizer* qui transforme un document en une matrice de comptage pour chaque mot du texte. Une ligne correspond à un document et une colonne à un mot. Attention, cela créée des matrices sparses (creuses) donc remplies de zéro, Numpy permet de stocker sous un format optimisé ce type de matrice (optimisé en taille de stockage et temps d'accès)\n",
    "\n",
    "+ *TF/IDF* partant du comptage, on va passer aux fréquence, un mot fréquent est moins discriminant ou caractéristique qu'un mot rare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40aee81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# on encode la variable à prédire en une variable binaire \n",
    "encode_y = LabelEncoder()\n",
    "y = encode_y.fit_transform(sms['class'])\n",
    "\n",
    "# On sépare en données d'entraînement et de test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(sms['sms'], y, test_size=0.2)\n",
    "\n",
    "# On applique tf_idf ensuite !\n",
    "from sklearn.feature_selection.text import TfidfVectorizer\n",
    "trans_vect = TfidfVectorizer()\n",
    "\n",
    "x_train_trans = trans_vect.fit_transform(x_train)\n",
    "x_test_trans = trans_vect.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced8ad94",
   "metadata": {},
   "source": [
    "On va ensuite applqiuer des classifieurs classiques (pas des réseaux de neurones mais efficace quand même en général !)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a21fa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# on instantie les modèles\n",
    "bayes = MultinomialNB()\n",
    "svm = SVC()\n",
    "\n",
    "bayes.fit(x_train_trans, y_train)\n",
    "svm.fit(x_train_trans, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06131dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Il faut ensuite vérifier la qualité des modèles\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "print(f'Accuracy pour le modèle NaiveBayes : {accuracy_score(y_test, bayes.predict(x_test_trans))}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a4d6a8",
   "metadata": {},
   "source": [
    "Comparer au modèle SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d6e968",
   "metadata": {},
   "source": [
    "Dans une logique de mise en production, donc d'automatisation, on va construire une pipeline de traitement pour de futurs messages arrivant : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336b9588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "\n",
    "# créons une pipeline de traitement du text\n",
    "pipe_text = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "\n",
    "# On réentraine le modèle sur toutes les données disponibles\n",
    "pipe_text.fit(sms['sms'], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b27c071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va créer une fonction de filtre qui affiche un message si elle détecte un spam\n",
    "\n",
    "def filtre_message(message):\n",
    "    arr_mess = np.array([messsage])\n",
    "    result = encode_y.inverse_transform(pipe_text.predict(arr_mess))[0]\n",
    "    print(f'Ce message est : {result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be676ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testons la fonction avec un nouveau message\n",
    "\n",
    "filtre_message(\"Urgent, You are a winner\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
