{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "    <td width=25%>\n",
    "        <img src=\"images/cytech_logo.png\">\n",
    "    </td>\n",
    "    <td>\n",
    "        <center>\n",
    "            <h1>Deep Learning et Applications</h1>\n",
    "        </center>\n",
    "    </td>\n",
    "    <td width=15%>\n",
    "        Yann Vernaz \n",
    "    </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<br/><br/>\n",
    "<center>\n",
    "    <a style=\"font-size: 20pt; font-weight: bold\">Lab. 5 - Application : Recherche visuelle</a>\n",
    "</center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif de ce <i>Lab.</i> est d'utiliser un modèle pré-entraîné (i.e. `ResNet50`) pour extraire les représentations des images afin de construire un outil pour rechercher des images similaires dans une base d'images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "from zipfile import ZipFile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "# extraction des données (PASCAL VOC)\n",
    "if not os.path.exists(\"images_resize\"):\n",
    "    print('Extraction des images ... dans images_resize/')\n",
    "    zf = ZipFile('images_pascalVOC.zip')\n",
    "    zf.extractall('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle pré-entraîné (ResNet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.applications.ResNet50(include_top=True, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification d'une image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib.pyplot import imread\n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "from tensorflow.keras.applications.imagenet_utils import decode_predictions\n",
    "\n",
    "image_path = \"images_resize/000007.jpg\"\n",
    "\n",
    "img = imread(image_path)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(img)\n",
    "\n",
    "img = cv2.resize(img, (224,224)).astype(\"float32\")\n",
    "img_batch = preprocess_input(img[np.newaxis]) \n",
    "\n",
    "predictions = model.predict(img_batch)\n",
    "decoded_predictions= decode_predictions(predictions)\n",
    "\n",
    "for s, predicted_class, score in decoded_predictions[0]:\n",
    "    print(\"{0} \\t {1}%\".format(predicted_class, round(100*score,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcul de la représentation des images\n",
    "\n",
    "À présent nous allons extraire du réseau la représentation vectorielle des images. Cette représentation correspond à la sortie de la dernière couche du réseau `ResNet50`avant la dernière étape de classification (<i>softmax</i>). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = model.layers[0].input\n",
    "output_ = model.layers[-2].output\n",
    "base_model = tf.keras.models.Model(input_, output_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representation = base_model.predict(img_batch)\n",
    "print(\"Dimension de la représentation : {0}\".format(representation.shape))\n",
    "print(\"representation={0}\".format(representation))\n",
    "print(\"\\nProportion de valeurs à zéro (pas d'activation) : {0}%\".format(round(100*np.mean(representation[0]==0),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une image est donc représentée par un vecteur dense de taille 2048. On peut voir que presque 10% des valeurs sont nulles (pas d'activation).\n",
    "\n",
    "**NOTE**\n",
    "\n",
    "Le calcul des représentations de toutes les images peut prendre du temps. Elles sont généralement calculées par lots sur GPU. Nous utiliserons des représentations pré-calculées enregistrées au format h5. Pour les personnes intéressées, cela se fait à l'aide du script `process_images.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_paths = [\"images_resize/\" + path for path in sorted(os.listdir(\"images_resize/\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des représentations pré-calculées\n",
    "h5f = h5py.File('images_embedding.h5','r')\n",
    "out_tensors = h5f['img_emb'][:]\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les représentations sont denses.\n",
    "\n",
    "<hr style=\"height:3px;border-top:1px solid #fff\" />\n",
    "\n",
    "> **EXRECICE 1**\n",
    ">\n",
    "> - Quelle proportion des représentations sont égales à 0 ?<br/><br/>\n",
    ">\n",
    "> - Pourquoi y a-t-il des valeurs nulles ?\n",
    ">\n",
    "\n",
    "<hr style=\"height:3px;border-top:1px solid #fff\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Votre code ici\n",
    "## ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization t-SNE\n",
    "\n",
    "La méthode t-Distributed Stochastic Neighbor Embedding (t-SNE) [1] est une réduction de dimension non linéaire, dont l’objectif est d’assurer que des points proches dans l’espace de départ gardent des positions proches dans l’espace projeté (2D). Dit autrement, la mesure de distance entre points dans l’espace projecté 2D doit refléter la mesure de distance dans l’espace initial.\n",
    "\n",
    "[1] Laurens van der Maaten and Geoffrey E. Hinton. Visualizing high-dimensional data using t-sne. Journal of Machine Learning Research, 9:2579–2605, 2008."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "images_embedding_tsne = TSNE(perplexity=30).fit_transform(out_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.title(\"Visualisation t-SNE\")\n",
    "plt.scatter(images_embedding_tsne[:, 0], images_embedding_tsne[:, 1]);\n",
    "plt.xticks(()); plt.yticks(());\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajoutons les vignettes des images originales dans la visualisation `t-SNE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imscatter(x, y, paths, ax=None, zoom=1, linewidth=0):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    x, y = np.atleast_1d(x, y)\n",
    "    artists = []\n",
    "    for x0, y0, p in zip(x, y, paths):\n",
    "        try:\n",
    "            im = imread(p)\n",
    "        except:\n",
    "            print(p)\n",
    "            continue\n",
    "        im = cv2.resize(im,(224,224))\n",
    "        im = OffsetImage(im, zoom=zoom)\n",
    "        ab = AnnotationBbox(im, (x0, y0), xycoords='data',\n",
    "                            frameon=True, pad=0.1, \n",
    "                            bboxprops=dict(edgecolor='red',\n",
    "                                           linewidth=linewidth))\n",
    "        artists.append(ax.add_artist(ab))\n",
    "    ax.update_datalim(np.column_stack([x, y]))\n",
    "    ax.autoscale()\n",
    "    return artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(50, 50))\n",
    "plt.title(\"Visualisation t-SNE\")\n",
    "imscatter(images_embedding_tsne[:, 0], images_embedding_tsne[:, 1], paths, zoom=0.5, ax=ax)\n",
    "plt.savefig('tsne.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recherche visuelle\n",
    "\n",
    "Nous allons rechercher les images les plus proches (similaires) en utilisant comme distance la norme $L_2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilitaire pour afficher une image \n",
    "def display_image(image):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(imread(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-top:1px solid #fff\" />\n",
    "\n",
    "> **EXRECICE 2**\n",
    ">\n",
    "> Implémentez une fonction qui calcule la distance entre un image et toutes les autres<br/><br/>\n",
    ">\n",
    "> Utilisez la fonction `np.linalg.norm`.\n",
    ">\n",
    "\n",
    "<hr style=\"height:3px;border-top:1px solid #fff\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_images(image_index, top_n=5):\n",
    "    \n",
    "    # distances entre les images\n",
    "    dists = ## Votre code ici\n",
    "            ## ...\n",
    "        \n",
    "    sorted_dists = np.argsort(dists)\n",
    "    return sorted_dists[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "image_index = 57\n",
    "\n",
    "images_similar = most_similar_images(image_index)\n",
    "\n",
    "# affichage \n",
    "result = [display_image(images_paths[image]) for image in images_similar]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification avec les plus proches voisins (Nearest Neighbors) ?\n",
    "\n",
    "En utilisant ces représentations, on peut construire un classifieur [Nearest Neighbor] (https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm). Cependant, les représentations apprises sur `ImageNet`, qui sont uniquement des images centrées, contrairement aux images de `PascalVOC` qui sont plus générales.\n",
    "\n",
    "Nous explorons cette possibilité en calculant l'histogramme des similitudes entre une image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm L2\n",
    "out_norms = np.linalg.norm(out_tensors, axis=1, keepdims=True)\n",
    "\n",
    "# normalisation\n",
    "normed_out_tensors = out_tensors / out_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index = 208\n",
    "\n",
    "# distances \n",
    "dists_to_item = np.linalg.norm(out_tensors - out_tensors[image_index], axis=1)\n",
    "\n",
    "# cosinus similitude\n",
    "cos_to_item = np.dot(normed_out_tensors, normed_out_tensors[image_index])\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title(\"Histogramme des similitudes\")\n",
    "plt.hist(cos_to_item, bins=20)\n",
    "display_image(images_paths[image_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = np.where(cos_to_item > 0.44)\n",
    "print(items)\n",
    "result = [display_image(paths[s]) for s, _ in zip(items[0], range(10))];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Malheureusement, il n'y a pas de séparation évidente des frontières de classe visible sur l'histogramme des similitudes. Nous avons besoin d'une certaine supervision pour pouvoir classer les images.\n",
    "\n",
    "<hr style=\"height:3px;border-top:1px solid #fff\" />\n",
    "\n",
    "> **EXERCICE BONUS**\n",
    ">\n",
    "> Avec un ensemble de données étiquetées, même avec très peu d'étiquettes par classe, on pourrait le faire :\n",
    "> \n",
    "> - construire un modèle k-Nearest Neighbor,\n",
    "> \n",
    "> - construire un modèle de classification [SVM](https://scikit-learn.org/stable/modules/svm.html).\n",
    "> \n",
    "> Conclure\n",
    "\n",
    "<hr style=\"height:3px;border-top:1px solid #fff\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Votre solution ici\n",
    "## ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
